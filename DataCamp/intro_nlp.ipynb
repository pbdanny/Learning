{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Regex"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import re"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Write a pattern to match sentence endings: sentence_endings\n",
                "sentence_endings = r\"[!.?]\"\n",
                "\n",
                "# Split my_string on sentence endings and print the result\n",
                "print(re.split(sentence_endings, my_string))\n",
                "\n",
                "# Find all capitalized words in my_string and print the result\n",
                "capitalized_words = r\"[A-Z]\\w+\"\n",
                "print(re.findall(capitalized_words, my_string))\n",
                "\n",
                "# Split my_string on spaces and print the result\n",
                "spaces = r\"\\s+\"\n",
                "print(re.split(spaces, my_string))\n",
                "\n",
                "# Find all digits in my_string and print the result\n",
                "digits = r\"\\d+\"\n",
                "print(re.findall(digits, my_string))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
                        "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
                        "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
                        "['4', '19']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "with open('../dataset/nlp_scene_one.txt', 'r') as f:\n",
                "    scene_one = f.read()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "# Import necessary modules\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "\n",
                "# Split scene_one into sentences: sentences\n",
                "sentences = sent_tokenize(scene_one)\n",
                "\n",
                "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
                "tokenized_sent = word_tokenize(sentences[3])\n",
                "\n",
                "# Make a set of unique tokens in the entire scene: unique_tokens\n",
                "unique_tokens = set(word_tokenize(scene_one))\n",
                "\n",
                "# Print the unique tokens result\n",
                "print(unique_tokens)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "{'weight', 'goes', 'Ridden', 'my', 'empty', 'south', 'sun', 'not', 'Oh', 'swallow', 'is', 'grip', 'search', 'but', 'son', 'The', 'speak', 'But', 'yeah', 'house', 'covered', 'Wait', 'then', 'Saxons', '!', 'line', 'defeator', 'that', 'length', 'all', 'SCENE', 'coconut', 'maybe', 'wind', 'sovereign', 'seek', 'use', 'guiding', \"'em\", 'Well', 'are', 'Am', 'bring', 'its', 'tropical', 'he', 'here', 'King', 'could', 'lord', 'We', 'times', 'ounce', 'minute', ':', 'breadth', 'warmer', 'Britons', 'anyway', ']', 'will', 'Uther', 'by', 'our', 'air-speed', 'grips', 'climes', 'carry', 'temperate', \"'s\", 'It', 'me', 'am', 'where', 'together', 'pound', \"'ve\", \"'\", 'bird', 'velocity', 'this', 'interested', 'to', 'agree', 'mean', 'under', '#', 'dorsal', 'these', 'SOLDIER', 'KING', 'What', 'snows', 'So', 'found', \"n't\", 'Supposing', 'Listen', 'bangin', 'husk', '[', 'No', 'who', 'on', 'That', 'at', 'swallows', 'Who', 'they', 'needs', 'every', 'other', 'Arthur', 'Found', 'of', 'European', 'coconuts', 'Pull', 'does', 'Where', 'strand', 'must', 'an', 'land', 'carried', 'African', 'from', ',', 'and', 'plover', 'why', \"'d\", 'halves', 'Will', '?', 'with', 'strangers', 'maintain', 'They', 'one', 'creeper', 'using', 'point', 'Yes', 'do', 'zone', 'Please', 'held', 'clop', 'matter', '--', 'A', 'knights', 'ratios', 'wings', 'Whoa', 'In', 'court', 'fly', 'Mercea', 'servant', 'simple', 'if', 'through', 'second', 'two', 'England', 'go', 'non-migratory', 'You', 'you', 'master', 'ridden', '2', 'Patsy', 'feathers', 'ARTHUR', 'Camelot', 'forty-three', 'Halt', '.', 'kingdom', 'your', 'migrate', 'it', 'martin', 'question', 'get', 'got', 'there', 'winter', 'a', 'ask', 'castle', 'back', 'Are', 'wants', 'horse', 'Court', 'them', 'course', 'carrying', 'five', 'have', 'since', 'in', 'yet', 'order', 'join', 'may', 'right', '1', 'or', '...', 'tell', 'the', \"'re\", 'I', 'trusty', 'suggesting', 'Not', 'be', 'beat', 'Pendragon', \"'m\", 'just'}\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
                "match = re.search(\"coconuts\", scene_one)\n",
                "\n",
                "# Print the start and end indexes of match\n",
                "print(match.start(), match.end())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "581 589\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "pattern1 = r\"\\[.*]\"\n",
                "print(re.search(pattern1, scene_one))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "<re.Match object; span=(10, 33), match='[wind] [clop clop clop]'>\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "pattern2 = r\"[\\w\\s#]+:\"\n",
                "print(re.match(pattern2, sentences[3]))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "from nltk.tokenize import TweetTokenizer, regexp_tokenize\n",
                "\n",
                "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
                " '#NLP is super fun! <3 #learning',\n",
                " 'Thanks @datacamp :) #nlp #python']"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# Define a regex pattern to find hashtags: pattern1\n",
                "pattern1 = r\"#\\w+\"\n",
                "# Use the pattern on the first tweet in the tweets list\n",
                "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
                "print(hashtags)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['#nlp', '#python']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "pattern2 = r\"([@#]\\w+)\"\n",
                "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
                "print(mentions_hashtags)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['@datacamp', '#nlp', '#python']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "tknzr = TweetTokenizer()\n",
                "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
                "print(all_tokens)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Unicode ranges for emoji are:\n",
                "\n",
                "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF').\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "all_words = word_tokenize(german_text)\n",
                "print(all_words)\n",
                "\n",
                "# Tokenize and print only capital words\n",
                "capital_words = r\"[A-ZÜ]\\w+\"\n",
                "print(regexp_tokenize(german_text, capital_words))\n",
                "\n",
                "# Tokenize and print only emoji\n",
                "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
                "print(regexp_tokenize(german_text, emoji))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
                        "['Wann', 'Pizza', 'Und', 'Über']\n",
                        "['🍕', '🚕']\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "from matplotlib import pyplot as plt\n",
                "\n",
                "# Split the script into lines: lines\n",
                "lines = scene_one.split('\\n')\n",
                "\n",
                "# Replace all script lines for speaker\n",
                "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
                "lines = [re.sub(pattern, '', l) for l in lines]\n",
                "\n",
                "# Tokenize each line: tokenized_lines\n",
                "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines]\n",
                "\n",
                "# Make a frequency list of lengths: line_num_words\n",
                "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
                "\n",
                "# Plot a histogram of the line lengths\n",
                "plt.hist(line_num_words)\n",
                "\n",
                "# Show the plot\n",
                "plt.show()"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Topics Identification"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "from nltk.tokenize import word_tokenize\n",
                "from collections import Counter, defaultdict\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk.corpus import stopwords\n",
                "import itertools"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "with open('../dataset/nlp_article.txt', 'r') as f:\n",
                "    article = f.read()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "tokens = word_tokenize(article)\n",
                "lower_tokens = [w.lower() for w in tokens]\n",
                "bow_simple = Counter(lower_tokens)\n",
                "print(bow_simple.most_common(10))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "from nltk import download\n",
                "download('stopwords')\n",
                "download('wordnet')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /Users/Danny/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to /Users/Danny/nltk_data...\n",
                        "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
                    ]
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "english_stops = stopwords.words('english')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
                "no_stops = [t for t in alpha_only if t not in english_stops]\n",
                "wordnet_lemmatizer = WordNetLemmatizer()\n",
                "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
                "bow = Counter(lemmatized)\n",
                "print(bow.most_common(10))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "from gensim.corpora import Dictionary"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "dictionary = Dictionary([lemmatized])\n",
                "file_id = dictionary.token2id.get('file')\n",
                "print(dictionary.get(file_id))\n",
                "\n",
                "corpus = [dictionary.doc2bow([article]) for article in [article]]"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "file\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "doc = corpus[0]\n",
                "\n",
                "# Sort the doc for frequency: bow_doc\n",
                "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
                "\n",
                "# Print the top 5 words of the document alongside the count\n",
                "for word_id, word_count in bow_doc[:5]:\n",
                "    print(dictionary.get(word_id), word_count)\n",
                "    \n",
                "# Create the defaultdict: total_word_count\n",
                "total_word_count = defaultdict(int)\n",
                "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
                "    total_word_count[word_id] += word_count"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "from gensim.models.tfidfmodel import TfidfModel"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "tfidf = TfidfModel(corpus)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "tfidf_weights = tfidf[doc]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "print(tfidf_weights[:5])\n",
                "\n",
                "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
                "\n",
                "for term_id, weight in sorted_tfidf_weights[:5]:\n",
                "    print(dictionary.get(term_id), weight)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Name Entity Recognition "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 137,
            "source": [
                "from collections import defaultdict\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk.corpus import stopwords\n",
                "from nltk import pos_tag\n",
                "from nltk import ne_chunk_sents\n",
                "\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "wordnet_lemmatizer = WordNetLemmatizer()\n",
                "eng_stopwords = stopwords.words('english')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 132,
            "source": [
                "with open('../dataset/nlp_ner_article.txt', 'r') as f:\n",
                "    article = f.read()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 149,
            "source": [
                "sentences = sent_tokenize(article)\n",
                "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
                "token_sentences_rm_punc = [[w for w in sent if w.isalpha()] for sent in token_sentences]\n",
                "token_sentences_rm_stop = [[w for w in sent if w not in eng_stopwords] for sent in token_sentences_rm_punc]\n",
                "token_sentences_stem = [[wordnet_lemmatizer.lemmatize(w) for w in sent] for sent in token_sentences_rm_stop]\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 150,
            "source": [
                "pos_sentences = [pos_tag(sent) for sent in token_sentences_stem]\n",
                "chunked_sentences = ne_chunk_sents(pos_sentences, binary=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for sent in chunked_sentences:\n",
                "    for chunk in sent:\n",
                "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
                "            print(chunk)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for sent in chunked_sentences:\n",
                "    for chunk in sent:\n",
                "        print(chunk)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 152,
            "source": [
                "chunked_sentences = ne_chunk_sents(pos_sentences)\n",
                "\n",
                "ner_categories = defaultdict(int)\n",
                "\n",
                "# count entity, stored in defaultdict\n",
                "for sent in chunked_sentences:\n",
                "    for chunk in sent:\n",
                "        if hasattr(chunk, 'label'):\n",
                "            ner_categories[chunk.label()] += 1"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 153,
            "source": [
                "labels, values = [], []\n",
                "for k, v in ner_categories.items():\n",
                "    labels.append(k)\n",
                "    values.append(v)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 154,
            "source": [
                "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
                "plt.show()"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {}
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# SpaCy"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import spacy"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "!python -m spacy download en_core_web_sm"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "nlp = spacy.load('en_core_web_sm')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "with open('../dataset/nlp_ner_article.txt', 'r') as f:\n",
                "    article = f.read()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "doc = nlp(article)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "for ent in doc.ents:\n",
                "    print(ent.label_, ent.text)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "ORG Apple\n",
                        "PERSON Uber\n",
                        "FAC Travis Kalanick of Uber\n",
                        "PERSON Tim Cook\n",
                        "ORG Apple\n",
                        "CARDINAL Millions\n",
                        "PERSON Uber\n",
                        "LOC Silicon Valley\n",
                        "ORG Yahoo\n",
                        "PERSON Marissa Mayer\n",
                        "MONEY 186\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "----"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Sklearn Bag of Word"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Import the necessary modules\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn import metrics\n",
                "\n",
                "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
                "nb_classifier = MultinomialNB()\n",
                "\n",
                "# Fit the classifier to the training data\n",
                "nb_classifier.fit(count_train, y_train)\n",
                "\n",
                "# Create the predicted tags: pred\n",
                "pred = nb_classifier.predict(count_test)\n",
                "\n",
                "# Calculate the accuracy score: score\n",
                "score = metrics.accuracy_score(pred, y_test)\n",
                "print(score)\n",
                "\n",
                "# Calculate the confusion matrix: cm\n",
                "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
                "print(cm)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Get the class labels: class_labels\n",
                "class_labels = nb_classifier.classes_\n",
                "\n",
                "# Extract the features: feature_names\n",
                "feature_names = tfidf_vectorizer.get_feature_names()\n",
                "\n",
                "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
                "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
                "\n",
                "# Print the first class label and the top 20 feat_with_weights entries\n",
                "print(class_labels[0], feat_with_weights[:20])\n",
                "\n",
                "# Print the second class label and the bottom 20 feat_with_weights entries\n",
                "print(class_labels[1], feat_with_weights[-20:])\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit ('base': conda)"
        },
        "interpreter": {
            "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}