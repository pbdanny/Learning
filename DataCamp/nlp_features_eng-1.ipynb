{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# NLP Features Engineering"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import pandas as pd\n",
                "import numpy as np"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "data = [[29.0, 0, 0, 211.3375, 'female', 1],\n",
                "       [0.9167, 1, 2, 151.55, 'male', 1],\n",
                "       [2.0, 1, 2, 151.55, 'female', 0],\n",
                "       [30.0, 1, 2, 151.55, 'male', 0],\n",
                "       [25.0, 1, 2, 151.55, 'female', 0],\n",
                "       [48.0, 0, 0, 26.55, 'male', 1],\n",
                "       [63.0, 1, 0, 77.9583, 'female', 1],\n",
                "       [39.0, 0, 0, 0.0, 'male', 0],\n",
                "       [53.0, 2, 0, 51.4792, 'female', 1],\n",
                "       [71.0, 0, 0, 49.5042, 'male', 0],\n",
                "       [47.0, 1, 0, 227.525, 'male', 0],\n",
                "       [18.0, 1, 0, 227.525, 'female', 1],\n",
                "       [24.0, 0, 0, 69.3, 'female', 1],\n",
                "       [26.0, 0, 0, 78.85, 'female', 1],\n",
                "       [80.0, 0, 0, 30.0, 'male', 1]]\n",
                "\n",
                "df1 = pd.DataFrame(data, columns=['feature 1', 'feature 2', 'feature 3', 'feature 4', 'feature 5', 'label'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "`pd.get_dummies()` with k-1 encoding use drop_first=True) = sklearn.preprocessing.LabelEncoder"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "pd.get_dummies(df1, columns=['feature 5'], drop_first=True)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "    feature 1  feature 2  feature 3  feature 4  label  feature 5_male\n",
                            "0     29.0000          0          0   211.3375      1               0\n",
                            "1      0.9167          1          2   151.5500      1               1\n",
                            "2      2.0000          1          2   151.5500      0               0\n",
                            "3     30.0000          1          2   151.5500      0               1\n",
                            "4     25.0000          1          2   151.5500      0               0\n",
                            "5     48.0000          0          0    26.5500      1               1\n",
                            "6     63.0000          1          0    77.9583      1               0\n",
                            "7     39.0000          0          0     0.0000      0               1\n",
                            "8     53.0000          2          0    51.4792      1               0\n",
                            "9     71.0000          0          0    49.5042      0               1\n",
                            "10    47.0000          1          0   227.5250      0               1\n",
                            "11    18.0000          1          0   227.5250      1               0\n",
                            "12    24.0000          0          0    69.3000      1               0\n",
                            "13    26.0000          0          0    78.8500      1               0\n",
                            "14    80.0000          0          0    30.0000      1               1"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>feature 1</th>\n",
                            "      <th>feature 2</th>\n",
                            "      <th>feature 3</th>\n",
                            "      <th>feature 4</th>\n",
                            "      <th>label</th>\n",
                            "      <th>feature 5_male</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>29.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>211.3375</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0.9167</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>151.5500</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>2.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>151.5500</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>30.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>151.5500</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>25.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>151.5500</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>48.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>26.5500</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>63.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>77.9583</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>39.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>53.0000</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0</td>\n",
                            "      <td>51.4792</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>71.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>49.5042</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>47.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>227.5250</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>18.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>227.5250</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>12</th>\n",
                            "      <td>24.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>69.3000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>26.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>78.8500</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>80.0000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>30.0000</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 6
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "tweet = pd.read_csv('../dataset/vaccination_tweets.csv')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Charactor count"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "# 1\n",
                "tweet['text'].apply(len)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0        97\n",
                            "1       140\n",
                            "2       140\n",
                            "3       140\n",
                            "4       135\n",
                            "       ... \n",
                            "9441     77\n",
                            "9442    140\n",
                            "9443    140\n",
                            "9444    140\n",
                            "9445    110\n",
                            "Name: text, Length: 9446, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 14
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# 2\n",
                "tweet['text'].str.len()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0        97\n",
                            "1       140\n",
                            "2       140\n",
                            "3       140\n",
                            "4       135\n",
                            "       ... \n",
                            "9441     77\n",
                            "9442    140\n",
                            "9443    140\n",
                            "9444    140\n",
                            "9445    110\n",
                            "Name: text, Length: 9446, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 15
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Word count"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "#1\n",
                "tweet['text'].str.split(' ').str.len()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0       12\n",
                            "1       21\n",
                            "2       15\n",
                            "3       20\n",
                            "4       14\n",
                            "        ..\n",
                            "9441    11\n",
                            "9442    16\n",
                            "9443    15\n",
                            "9444    17\n",
                            "9445    11\n",
                            "Name: text, Length: 9446, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 21
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "#2\n",
                "def count_words(sent: str) -> int:\n",
                "    words = sent.split(' ')\n",
                "    return len(words)\n",
                "\n",
                "tweet['text'].apply(count_words)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0       12\n",
                            "1       21\n",
                            "2       15\n",
                            "3       20\n",
                            "4       14\n",
                            "        ..\n",
                            "9441    11\n",
                            "9442    16\n",
                            "9443    15\n",
                            "9444    17\n",
                            "9445    11\n",
                            "Name: text, Length: 9446, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Hashtag"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "def count_hastag(sent: str) -> int:\n",
                "    words = sent.split(' ')\n",
                "    hashed = [word for word in words if word.startswith('#')]\n",
                "    return len(hashed)\n",
                "\n",
                "def count_tag(sent: str) -> int:\n",
                "    words = sent.split(' ')\n",
                "    hashed = [word for word in words if word.startswith('@')]\n",
                "    return len(hashed)\n",
                "\n",
                "tweet['text'].apply(count_hastag)[:5]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0    1\n",
                            "1    0\n",
                            "2    6\n",
                            "3    0\n",
                            "4    2\n",
                            "Name: text, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 16
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "tweet['text'].apply(count_tag)[:5]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0    0\n",
                            "1    0\n",
                            "2    0\n",
                            "3    0\n",
                            "4    2\n",
                            "Name: text, dtype: int64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 18
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import textstat"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "text = 'In multilabel learning, the joint set of binary classification tasks is expressed with a label binary indicator array: each sample is one row of a 2d array of shape (n_samples, n_classes) with binary values where the one, i.e. the non zero elements, corresponds to the subset of labels for that sample. An array such as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the first sample, labels 1 and 2 in the second sample, and no labels in the third sample.'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "flesh_score = textstat.flesch_reading_ease(text)\n",
                "gunn_score = textstat.gunning_fog(text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "flesh_score"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "36.29"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "gunn_score"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "20.46"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Tokenization & Lemmatization"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "import spacy\n",
                "with open('../dataset/nlp_ner_article.txt', 'r') as f:\n",
                "    sentences = f.read()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "source": [
                "nlp = spacy.load('en_core_web_sm')\n",
                "doc = nlp(sentences)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "for token in doc[:10]:\n",
                "    print(token.text)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "The\n",
                        "taxi\n",
                        "-\n",
                        "hailing\n",
                        "company\n",
                        "Uber\n",
                        "brings\n",
                        "into\n",
                        "very\n",
                        "sharp\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "for token in doc[:10]:\n",
                "    print(token.lemma_)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "the\n",
                        "taxi\n",
                        "-\n",
                        "hail\n",
                        "company\n",
                        "Uber\n",
                        "bring\n",
                        "into\n",
                        "very\n",
                        "sharp\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "en_stop = spacy.lang.en.stop_words.STOP_WORDS"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "lemmas = [token.lemma_ for token in doc]\n",
                "lemmas_a = [lemma for lemma in lemmas if lemma.isalpha() or lemma not in en_stop]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "lemmas_a_1 = [token.lemma_ for token in doc if token.lemma_.isalpha() or token.lemma_ not in en_stop]\n",
                "lemmas_a_2 = [token.lemma_ if (token.lemma_.isalpha() or token.lemma_ not in en_stop) else None for token in doc]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Part Of Speech (POS) tagging"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "source": [
                "pos = [(token.text, token.pos_) for token in doc]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "source": [
                "print(pos[:10])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[('The', 'DET'), ('taxi', 'NOUN'), ('-', 'PUNCT'), ('hailing', 'VERB'), ('company', 'NOUN'), ('Uber', 'PROPN'), ('brings', 'VERB'), ('into', 'ADP'), ('very', 'ADV'), ('sharp', 'ADJ')]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "source": [
                "def nouns(text: str, model=nlp) -> int:\n",
                "  \t# Create doc object\n",
                "    doc = model(text)\n",
                "    # Generate list of POS tags\n",
                "    pos = [token.pos_ for token in doc]\n",
                "    \n",
                "    # Return number of other nouns\n",
                "    return pos.count(\"NOUN\")\n",
                "print(nouns(sentences))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "93\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Name Entity Recognition (NER)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "source": [
                "for ent in doc.ents:\n",
                "    print(ent.text, ent.label_)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Apple ORG\n",
                        "Uber PERSON\n",
                        "Travis Kalanick of Uber FAC\n",
                        "Tim Cook PERSON\n",
                        "Apple ORG\n",
                        "Millions CARDINAL\n",
                        "Uber PERSON\n",
                        "Silicon Valley LOC\n",
                        "Yahoo ORG\n",
                        "Marissa Mayer PERSON\n",
                        "186 MONEY\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
                "persons"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['Uber', 'Tim Cook', 'Uber', 'Marissa Mayer']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 57
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## N-Gram , BoW model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 117,
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "\n",
                "\n",
                "cv = CountVectorizer(lowercase=False, stop_words='english', ngram_range=(1,2))\n",
                "clf = MultinomialNB()\n",
                "\n",
                "synop = pd.read_csv('../dataset/Anime_Top10000.csv') #, usecols=['Anime_Name', 'Synopsis'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "source": [
                "\n",
                "synop.head()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "                           Anime_Name Anime_Episodes      Anime_Air_Years  \\\n",
                            "0    Fullmetal Alchemist: Brotherhood    TV (64 eps)  Apr 2009 - Jul 2010   \n",
                            "1  Shingeki no Kyojin Season 3 Part 2    TV (10 eps)  Apr 2019 - Jul 2019   \n",
                            "2                         Steins;Gate    TV (24 eps)  Apr 2011 - Sep 2011   \n",
                            "3                            Gintama°    TV (51 eps)  Apr 2015 - Mar 2016   \n",
                            "4              Hunter x Hunter (2011)   TV (148 eps)  Oct 2011 - Sep 2014   \n",
                            "\n",
                            "   Anime_Rating                                           Synopsis  label  \n",
                            "0          9.18  \"In order for something to be obtained, someth...   True  \n",
                            "1          9.11  Seeking to restore humanity's diminishing hope...   True  \n",
                            "2          9.11  The self-proclaimed mad scientist Rintarou Oka...   True  \n",
                            "3          9.09  Gintoki, Shinpachi, and Kagura return as the f...   True  \n",
                            "4          9.08  Hunter x Hunter is set in a world where Hunter...   True  "
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>Anime_Name</th>\n",
                            "      <th>Anime_Episodes</th>\n",
                            "      <th>Anime_Air_Years</th>\n",
                            "      <th>Anime_Rating</th>\n",
                            "      <th>Synopsis</th>\n",
                            "      <th>label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Fullmetal Alchemist: Brotherhood</td>\n",
                            "      <td>TV (64 eps)</td>\n",
                            "      <td>Apr 2009 - Jul 2010</td>\n",
                            "      <td>9.18</td>\n",
                            "      <td>\"In order for something to be obtained, someth...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Shingeki no Kyojin Season 3 Part 2</td>\n",
                            "      <td>TV (10 eps)</td>\n",
                            "      <td>Apr 2019 - Jul 2019</td>\n",
                            "      <td>9.11</td>\n",
                            "      <td>Seeking to restore humanity's diminishing hope...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Steins;Gate</td>\n",
                            "      <td>TV (24 eps)</td>\n",
                            "      <td>Apr 2011 - Sep 2011</td>\n",
                            "      <td>9.11</td>\n",
                            "      <td>The self-proclaimed mad scientist Rintarou Oka...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Gintama°</td>\n",
                            "      <td>TV (51 eps)</td>\n",
                            "      <td>Apr 2015 - Mar 2016</td>\n",
                            "      <td>9.09</td>\n",
                            "      <td>Gintoki, Shinpachi, and Kagura return as the f...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Hunter x Hunter (2011)</td>\n",
                            "      <td>TV (148 eps)</td>\n",
                            "      <td>Oct 2011 - Sep 2014</td>\n",
                            "      <td>9.08</td>\n",
                            "      <td>Hunter x Hunter is set in a world where Hunter...</td>\n",
                            "      <td>True</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 161
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 118,
            "source": [
                "synop['label'] = synop['Anime_Rating'] > synop['Anime_Rating'].mean()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 119,
            "source": [
                "bow = cv.fit_transform(synop['Synopsis'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "source": [
                "bow_df = pd.DataFrame(data=bow.toarray())\n",
                "bow_df.columns = cv.get_feature_names()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 120,
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(bow, synop['label'], stratify=synop['label'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 121,
            "source": [
                "clf.fit(X_train, y_train)\n",
                "clf.score(X_train, y_train)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0.9701333333333333"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 121
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 122,
            "source": [
                "clf.score(X_test, y_test)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "0.708"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 122
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## TF-IDF Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances, linear_kernel"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "source": [
                "np.array([1,3]).dot(np.array([-2,2]))"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 134
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 135,
            "source": [
                "np.dot(np.array([1,3]), np.array([-2,2]))"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "4"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 135
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 148,
            "source": [
                "np.array([1,3]).reshape(1,-1)@np.array([[-2],[2]])"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([[4]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 148
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 209,
            "source": [
                "tfidf_vec = TfidfVectorizer(stop_words='english', max_df=0.5)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 210,
            "source": [
                "tfidf = tfidf_vec.fit_transform(synop['Synopsis'])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 211,
            "source": [
                "cosin_sim = cosine_similarity(tfidf, tfidf)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 212,
            "source": [
                "cosin_dist = cosine_distances(tfidf, tfidf)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 213,
            "source": [
                "# Cosine dist = 1 - cosine sim\n",
                "cosin_sim + cosin_dist"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "array([[1., 1., 1., ..., 1., 1., 1.],\n",
                            "       [1., 1., 1., ..., 1., 1., 1.],\n",
                            "       [1., 1., 1., ..., 1., 1., 1.],\n",
                            "       ...,\n",
                            "       [1., 1., 1., ..., 1., 1., 1.],\n",
                            "       [1., 1., 1., ..., 1., 1., 1.],\n",
                            "       [1., 1., 1., ..., 1., 1., 1.]])"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 213
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "pair-wise liner kernel, same result as cosine similarity but `Faster`"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 214,
            "source": [
                "cosin_sim = linear_kernel(tfidf, tfidf)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 215,
            "source": [
                "indices = pd.Series(data=synop.index, index=synop['Anime_Name']).drop_duplicates()\n",
                "\n",
                "def get_recommendation(title: str, cosine_sim: np.ndarray, indices: pd.Series):\n",
                "    # Get index of movie that matches title\n",
                "    idx = indices[title]\n",
                "    # Sort the movies based on the similarity scores\n",
                "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
                "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
                "    # Get the scores for 10 most similar movies\n",
                "    sim_scores = sim_scores[1:11]\n",
                "    # Get the movie indices\n",
                "    movie_indices = [i[0] for i in sim_scores]\n",
                "    # Return the top 10 most similar movies\n",
                "    return synop['Anime_Name'].iloc[movie_indices]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 216,
            "source": [
                "get_recommendation('Hunter x Hunter (2011)', cosin_sim, indices)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "149                               Hunter x Hunter\n",
                            "8266                              Grick no Bouken\n",
                            "296                 Hunter x Hunter: Greed Island\n",
                            "5290                                Dagashi Kashi\n",
                            "2601       Hunter x Hunter Movie 1: Phantom Rouge\n",
                            "197     Hunter x Hunter: Original Video Animation\n",
                            "9048           Chouon Senshi Borgman: Lovers Rain\n",
                            "9051    Digimon Adventure 3D: Digimon Grand Prix!\n",
                            "2379                                  Angel Heart\n",
                            "2602    Hunter x Hunter Movie 2: The Last Mission\n",
                            "Name: Anime_Name, dtype: object"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 216
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 217,
            "source": [
                "movies0_tfidf_label = pd.Series(data=tfidf[0].toarray().ravel(), index=tfidf_vec.get_feature_names())"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 218,
            "source": [
                "movies0_tfidf_label.sort_values(ascending=False)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "edward         0.406860\n",
                            "alphonse       0.352760\n",
                            "alchemist      0.248772\n",
                            "brothers       0.188060\n",
                            "philosopher    0.182620\n",
                            "                 ...   \n",
                            "gloomy         0.000000\n",
                            "gloria         0.000000\n",
                            "glories        0.000000\n",
                            "glorious       0.000000\n",
                            "黄色いしあわせ        0.000000\n",
                            "Length: 37827, dtype: float64"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 218
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Word embeddings (Word2Vec)\n",
                "\n",
                "Word embeddings add the meaning in to vector distance  \n",
                "Then same length sentence like 'I am happy' - 'I am sad' TF-IDF show close similarity, but with embeddings will show less / inverse similarity  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "!python -m spacy download en_core_web_md"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import spacy\n",
                "\n",
                "nlp = spacy.load('en_core_web_md')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "sent = 'I like and hate durian at the sametime'"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "doc = nlp(sent)\n",
                "for token1 in doc:\n",
                "    for token2 in doc:\n",
                "        print(token1.text, token2.text, token1.similarity(token2))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "I I 1.0\n",
                        "I like 0.5554912\n",
                        "I apples 0.20442726\n",
                        "I and 0.31607857\n",
                        "I oranges 0.18824081\n",
                        "like I 0.5554912\n",
                        "like like 1.0\n",
                        "like apples 0.32987142\n",
                        "like and 0.5267484\n",
                        "like oranges 0.2771747\n",
                        "apples I 0.20442726\n",
                        "apples like 0.32987142\n",
                        "apples apples 1.0\n",
                        "apples and 0.24097733\n",
                        "apples oranges 0.77809423\n",
                        "and I 0.31607857\n",
                        "and like 0.5267484\n",
                        "and apples 0.24097733\n",
                        "and and 1.0\n",
                        "and oranges 0.19245948\n",
                        "oranges I 0.18824081\n",
                        "oranges like 0.2771747\n",
                        "oranges apples 0.77809423\n",
                        "oranges and 0.19245948\n",
                        "oranges oranges 1.0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.2",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.2 64-bit ('nlp': conda)"
        },
        "interpreter": {
            "hash": "efa9c07e6be8469ee7c9ea39b91bbc240d6c784167c0da39bb2b4e6eb8040a1c"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}